import math
import pandas as pd
import urllib.request
import json
from scoring import models
from typing import List

RESULT_FILE_PATH = '../exp_out2/all/'
ROLE1 = [7, 12]
ROLE2 = [1, 2, 5, 6]
ROLE3 = [8, 9, 10]
ROLE4 = [3, 4, 11]


# This implementation is ad-hoc for writing paper.
def ip(df: pd.DataFrame, test_data_id_list: List[int]) -> List[float]:
    # ip@0, 0.1, 0.2, 0.3, 0.4, 0.5 0.6 0.7 0.8 0.9 1
    ip_list = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    l = df.shape[0]
    rel_size = len(test_data_id_list)
    rel_cnt = 0
    for i in range(0, l):
        if df.iloc[i].name in test_data_id_list:
            rel_cnt += 1
        recall = rel_cnt / rel_size
        pre = rel_cnt / (i+1)
        if recall >= 0:
            ip_list[0] = max(ip_list[0], pre)
        if recall >= 0.1:
            ip_list[1] = max(ip_list[1], pre)
        if recall >= 0.2:
            ip_list[2] = max(ip_list[2], pre)
        if recall >= 0.3:
            ip_list[3] = max(ip_list[3], pre)
        if recall >= 0.4:
            ip_list[4] = max(ip_list[4], pre)
        if recall >= 0.5:
            ip_list[5] = max(ip_list[5], pre)
        if recall >= 0.6:
            ip_list[6] = max(ip_list[6], pre)
        if recall >= 0.7:
            ip_list[7] = max(ip_list[7], pre)
        if recall >= 0.8:
            ip_list[8] = max(ip_list[8], pre)
        if recall >= 0.9:
            ip_list[9] = max(ip_list[9], pre)
        if recall >= 1:
            ip_list[10] = max(ip_list[10], pre)

    maip = sum(ip_list)/len(ip_list)
    ip_list.append(maip)
    return ip_list


def precision(df: pd.DataFrame, n: int, test_data_id_list: List[int]):
    top_n = df.head(n)
    count = 0
    for test_data_id in test_data_id_list:
        if not top_n[top_n.index == test_data_id].empty:
            count += 1
    if n == 30:
        for i in range(0, n):
            if top_n.iloc[i].name in test_data_id_list:
                print(i, top_n.iloc[i].name)
    return count / n


def n_dcg(df: pd.DataFrame, n: int, test_data_id_list:List[int]):
    topn = df.head(n)

    rels = []
    for i in range(0, n):
        if topn.iloc[i].name in test_data_id_list:
            rels.append(1)
        else:
            rels.append(0)

    dcg = 0
    for i, rel in enumerate(rels):
        index = i + 1
        dcg += (2 ** rel - 1) / math.log2(index + 1)

    irels = sorted(rels, reverse=True)
    idcg = 0
    for i, rel in enumerate(irels):
        index = i + 1
        idcg += (2 ** rel - 1) / math.log2(index + 1)

    if idcg == 0:
        return 0

    return dcg / idcg


def calc_recall(df: pd.DataFrame, n: int, test_data_id_list: List[int]):
    top_n = df.head(n)
    count = 0
    for test_data_id in test_data_id_list:
        if not top_n[top_n.index == test_data_id].empty:
            count += 1
    return count / len(test_data_id_list)


def adhoc_task(df: pd.DataFrame, n: int, test_data_id_list: List[int]) -> List[bool]:
    top_n = df.head(n)
    rels = []
    for i in range(0, n):
        if top_n.iloc[i].name in test_data_id_list:
            rels.append(True)
        else:
            rels.append(False)
    return rels


def adhoc_testing_task(file_name: str, labels: List[bool]):
    l = len(labels)
    fout = open(file_name, 'wt')
    num_label = ['1' if lbl else '0' for lbl in labels]
    fout.write(",".join(num_label))
    fout.write('\n')#org
    fout.close()


# モデルの評価を行う関数
# モデルクラスに各ユーザごとに訓練→　評価を繰り返し、最後にそれらの平均をファイルに出力する
def do_measure(score_model: models.ScoreModel):
    user_id_range = range(1, 13)
    user_count = len(user_id_range)

    method_measures_dict = {}
    for i in user_id_range:
        all_items = pd.read_json("../data50/all_items.json")
        f = open("../data50/user" + str(1) + "_kfolded.json", 'r')
        kfolded_training_and_test_data_list = json.load(f)

        print('###########################################')
        print('user = ' + str(i))
        print('###########################################')
        respective_method_measures_dict = {}
        for training_and_test_data in kfolded_training_and_test_data_list:
            training_hotel_list = training_and_test_data['trainingTrue']
            training_false_hotel_list = training_and_test_data['trainingFalse']
            test_hotel_list = training_and_test_data['testTrue']

            score_model.train(pd.DataFrame.from_records(training_hotel_list),
                              pd.DataFrame.from_records(training_false_hotel_list), i)
            ranking_dict = score_model.calc_ranking(all_items)

            test_data_id_list = [test_data['id'] for test_data in test_hotel_list]

            for method, ranking in ranking_dict.items():
                ranking_df = ranking
                ranking_df = ranking_df.drop([training_data['id'] for training_data in training_hotel_list])
                ranking_df = ranking_df.drop([training_data['id'] for training_data in training_false_hotel_list])
                print(ranking_df)

                if method not in respective_method_measures_dict:
                    respective_method_measures_dict[method] = {'ips': [0,0,0,0,0,0], 'maip': [0], 'ndcgs': [0,0,0,0,0], 'pres': [0,0,0,0,0],
                                                               'label10': [], 'label20': [], 'label30': []}

                ips = ip(ranking_df, test_data_id_list)
                respective_method_measures_dict[method]['ips'][0] += ips[0]
                respective_method_measures_dict[method]['ips'][1] += ips[1]
                respective_method_measures_dict[method]['ips'][2] += ips[2]
                respective_method_measures_dict[method]['ips'][3] += ips[3]
                respective_method_measures_dict[method]['ips'][4] += ips[4]
                respective_method_measures_dict[method]['ips'][5] += ips[5]

                respective_method_measures_dict[method]['maip'][0] += ips[11]

                respective_method_measures_dict[method]['ndcgs'][0] += n_dcg(ranking_df, 5, test_data_id_list)
                respective_method_measures_dict[method]['ndcgs'][1] += n_dcg(ranking_df, 10, test_data_id_list)
                respective_method_measures_dict[method]['ndcgs'][2] += n_dcg(ranking_df, 15, test_data_id_list)
                respective_method_measures_dict[method]['ndcgs'][3] += n_dcg(ranking_df, 20, test_data_id_list)
                respective_method_measures_dict[method]['ndcgs'][4] += n_dcg(ranking_df, 30, test_data_id_list)

                respective_method_measures_dict[method]['pres'][0] += precision(ranking_df, 5, test_data_id_list)
                respective_method_measures_dict[method]['pres'][1] += precision(ranking_df, 10, test_data_id_list)
                respective_method_measures_dict[method]['pres'][2] += precision(ranking_df, 15, test_data_id_list)
                respective_method_measures_dict[method]['pres'][3] += precision(ranking_df, 20, test_data_id_list)
                respective_method_measures_dict[method]['pres'][4] += precision(ranking_df, 30, test_data_id_list)

                respective_method_measures_dict[method]['label10'].extend(adhoc_task(ranking_df, 10, test_data_id_list))
                respective_method_measures_dict[method]['label20'].extend(adhoc_task(ranking_df, 20, test_data_id_list))
                respective_method_measures_dict[method]['label30'].extend(adhoc_task(ranking_df, 30, test_data_id_list))

        k = len(kfolded_training_and_test_data_list)
        for method, respective_measures in respective_method_measures_dict.items():
            if method not in method_measures_dict:
                method_measures_dict[method] = {'ips': [0,0,0,0,0,0], 'maip': [0], 'ndcgs': [0,0,0,0,0], 'pres': [0,0,0,0,0],
                                                'label10': [], 'label20': [], 'label30': []}

            for i in range(0, len(respective_measures['ips'])):
                respective_measures['ips'][i] /= k
                method_measures_dict[method]['ips'][i] += respective_measures['ips'][i]
            for i in range(0, len(respective_measures['maip'])):
                respective_measures['maip'][i] /= k
                method_measures_dict[method]['maip'][i] += respective_measures['maip'][i]
            for i in range(0, len(respective_measures['ndcgs'])):
                respective_measures['ndcgs'][i] /= k
                method_measures_dict[method]['ndcgs'][i] += respective_measures['ndcgs'][i]
            for i in range(0, len(respective_measures['pres'])):
                respective_measures['pres'][i] /= k
                method_measures_dict[method]['pres'][i] += respective_measures['pres'][i]
            method_measures_dict[method]['label10'].extend(respective_measures['label10'])
            method_measures_dict[method]['label20'].extend(respective_measures['label20'])
            method_measures_dict[method]['label30'].extend(respective_measures['label30'])
        f.close()
    for method, measures in method_measures_dict.items():
        file_name = RESULT_FILE_PATH + score_model.get_dirname() + "/"
        file_name += method + "_"
        file_name += score_model.get_modelname()
        fout = open(file_name, 'wt')
        fout.write('filename,')#org
        fout.write('ip0,ip0.1,ip0.2,ip0.3,ip0.4,ip0.5,maip,pre5,pre10,pre15,pre20,pre30,ndcg5,ndcg10,ndcg15,ndcg20,ndcg30\n')
        fout.write(str(file_name)+',')#org
        for i in range(0, len(measures['ips'])):
            measures['ips'][i] /= user_count
            fout.write(str(measures['ips'][i]) + ',')
        for i in range(0, len(measures['maip'])):
            measures['maip'][i] /= user_count
            fout.write(str(measures['maip'][i]) + ',')
        for i in range(0, len(measures['pres'])):
            measures['pres'][i] /= user_count
            fout.write(str(measures['pres'][i]) + ',')
        for i in range(0, len(measures['ndcgs'])):
            measures['ndcgs'][i] /= user_count
            if not i == 0:
                fout.write(",")
            fout.write(str(measures['ndcgs'][i]))
        fout.write('\n')#org
        fout.close()
        label10 = RESULT_FILE_PATH + score_model.get_dirname() + "/label10_" + method + "_" + score_model.get_modelname()
        label20 = RESULT_FILE_PATH + score_model.get_dirname() + "/label20_" + method + "_" + score_model.get_modelname()
        label30 = RESULT_FILE_PATH + score_model.get_dirname() + "/label30_" + method + "_" + score_model.get_modelname()
        adhoc_testing_task(label10, measures['label10'])
        adhoc_testing_task(label20, measures['label20'])
        adhoc_testing_task(label30, measures['label30'])
