from __future__ import division
from __future__ import print_function
from abc import ABCMeta, abstractmethod
from scipy.stats.kde import gaussian_kde
from scipy import integrate
from scipy.stats import norm
import pandas as pd
import math
from marginal import quantizer
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
from matplotlib.patches import Ellipse
import sys
import numpy as np
from sklearn import mixture
import warnings
from sklearn.neighbors.kde import KernelDensity
from sklearn.grid_search import GridSearchCV


warnings.filterwarnings('ignore')
prob=50  #probablity for kinnennScore

class Marginal(metaclass=ABCMeta):
    @abstractmethod
    def pdf(self, x: float) -> float:
        raise NotImplementedError

    @abstractmethod
    def cdf(self, x: float) -> float:
        raise NotImplementedError


class Kde(Marginal):
    def __init__(self, training_data_list: pd.Series, band=None):
        if((training_data_list==training_data_list[0]).all()):
            self.cor=0.000001
            self.cor *= 1 if training_data_list[0] < 1 else -1
            training_data_list[0]=training_data_list[0]+self.cor#mine
            self._kde = gaussian_kde(dataset=training_data_list, bw_method=band)
            training_data_list[0]-=self.cor#mine
        else:
            print(str(len(training_data_list))+'data')
            self._kde = gaussian_kde(dataset=training_data_list, bw_method=band)

    def pdf(self, single_point: float) -> float:
        return self._kde.evaluate([single_point])[0]

    def cdf(self, single_point: float) -> float:
        #x, e = integrate.quad(self.pdf, 0, single_point)
        x=self._kde.integrate_box_1d(-1000., single_point)#mine
        #print(x)
        return x

class Kdecv(Marginal):
    space='log'
    def __init__(self, training_data_list: pd.Series, bws, **arg):
        if Kdecv.space=='line':
            self.slcs=np.linspace(bws['0'],bws['1'],bws['2'])
        else:
            self.slcs=np.logspace(bws['0'],bws['1'],bws['2'])
        print(self.slcs)
        grid = GridSearchCV(KernelDensity(kernel='gaussian'),
        #{'bandwidth': np.linspace(bws['0'],bws['1'], bws['2'])},cv=len(training_data_list))
        {'bandwidth': self.slcs},cv=len(training_data_list))

        #print(np.linspace(bws['0'],bws['1'], bws['2']))
        #cv=len(training_data_list))#org
        print(training_data_list[:,None])
        grid.fit(training_data_list[:, None])
        
        score_type=arg['score_type']
        f=open('../cv'+str(prob)+'/'+score_type+str(bws['0'])+'-'+str(bws['1'])+'-'+str(bws['2']),'at')
        f.write('score,data_size,best,bws\n')
        self.best=grid.best_params_['bandwidth']
        print('bestbw:'+str(self.best))
        f.write(score_type+','+str(len(training_data_list))+','+str(self.best)+','+str(bws['0'])+'-'+str(bws['1'])+'-'+str(bws['2'])+'\n')
        f.close()
        #print(str(len(training_data_list))+'data')
        self._kde = gaussian_kde(dataset=training_data_list, bw_method=self.best)

    def pdf(self, single_point: float) -> float:
        return self._kde.evaluate([single_point])[0]

    def cdf(self, single_point: float) -> float:
        #x, e = integrate.quad(self.pdf, 0, single_point)
        x=self._kde.integrate_box_1d(-1000., single_point)#mine
        #print(x)
        return x



class Empirical(Marginal):
    def __init__(self, training_data_list: pd.Series):
        self.sr = training_data_list

    def pdf(self, x: float) -> float:
        raise NotImplementedError

    def cdf(self, x: float) -> float:
        s = self.sr
        return len(s[(s <= x)]) / len(s)


class Norm(Marginal):
    def __init__(self, training_data_list: pd.Series):
        self.mean = training_data_list.mean()
        self.sd = training_data_list.std()

    def pdf(self, single_point: float) -> float:
        return norm.pdf(x=single_point, loc=self.mean, scale=self.sd)

    def cdf(self, single_point: float) -> float:
        cdf = norm.cdf(x=single_point, loc=self.mean, scale=self.sd)
        if cdf is None:
            return 0
        if math.isnan(cdf):
            return 0
        return cdf


class QuantizedNorm(Marginal):#???
    def __init__(self, training_data_list: pd.Series):
        self.nrm = Norm(training_data_list)
        self.mean = self.nrm.mean
        self.sd = self.nrm.sd
        self.filter = quantizer.Representative(self.nrm)

    def pdf(self, single_point: float) -> float:
        return self.nrm.pdf(self.filter.get_representative(single_point))

    def cdf(self, single_point: float) -> float:
        return self.nrm.cdf(self.filter.get_representative(single_point))


class SimpleQuantizedNorm(Marginal):#???
    def __init__(self, training_data_list: pd.Series, value):
        self.nrm = Norm(training_data_list)
        self.mean = self.nrm.mean
        self.sd = self.nrm.sd
        self.filter = quantizer.SimpleRepresentative(self.nrm, value)

    def pdf(self, single_point: float) -> float:
        return self.nrm.pdf(self.filter.get_representative(single_point))

    def cdf(self, single_point: float) -> float:
        return self.nrm.cdf(self.filter.get_representative(single_point))
class EmNorm(Marginal):
    def __init__(self, training_data_list:pd.Series, component):
      data = training_data_list
      print(len(data))
      self.K = min([len(data), component])
      print(data)
      d_list = list(map(lambda x:[x],data))
      gmm = mixture.GMM(n_components = self.K, covariance_type='full', n_iter=1000)#random
      gmm.fit(d_list)
      self.pis = gmm.weights_.tolist()
      self.ms = flatten_with_any_depth(gmm.means_.tolist())
      self.covars = flatten_with_any_depth(gmm.covars_.tolist())
      self.vars = list(map(math.sqrt,self.covars))
      self.par = [self.pis, self.ms, self.vars]
      print('em succeed:'+str(self.par))
    def pdf(self, single_point: float) -> float:
        res=0
        for p, m, v in zip(*self.par):
            res+=norm.pdf(x=single_point, loc=m, scale=v)*p
        return res

    def cdf(self, single_point: float) -> float:
        res=0
        for p, m, v in zip(*self.par):
            cdf=p*norm.cdf(x=single_point, loc=m, scale=v)
            if cdf is None:
                print('cdferror:'+str(single_point)+':value:'+str(cdf))
                cdf = 0
            if math.isnan(cdf):
                print('cdferror'+str(single_point)+':value:'+str(cdf))
                cdf = 0
            res+=cdf
        #print('(point, cdf):'+str(single_point)+str(cdf))
        return res

#混合ガウス分布  par = (pi, mean, var): (混合係数、平均、分散)
def gaussians(x, par):
    #return [gaussian(x-mu, var) * pi for pi,mu,var in zip(*par)]
    res=[]
    for pi, mu, var in zip(*par):
        temp = pi*norm.pdf(x=x, loc=mu, scale=var)
        if temp == math.isnan(temp):
            """
            print('(x, pi, mu, var)->(nan)'+str(x)+' '+str(pi)+' '+str(mu)+' '+str(var)+' '+' '+str(temp))
            sys.exit()
            """
            temp=0
        if temp < 0:
            print('(x, pi, mu, var)->(nega)'+str(x)+' '+str(pi)+' '+str(mu)+' '+str(var)+' '+' '+str(temp))
            sys.exit()
            temp=0
        res.append(temp)
    return res
    #return [norm.pdf(x=x, loc=mu, scale=var) * pi for pi, mu, var in zip(*par)]

#ガウス分布
def gaussian(x, var):
    nvar = n_variate(x)
    if not nvar:
        qf, detvar, nvar = x**2/var, var, 1
    else:
        qf, detvar = np.dot(np.linalg.solve(var, x), x), np.linalg.det(var)
    return np.exp(-qf/2) / np.sqrt(detvar*(2*np.pi)**nvar)

#対数尤度
def loglikelihood(data, par):
    print('input')
    print(data)
    gam = [gaussians(x, par) for x in data]
    ll = sum([np.log(sum(g)) for g in gam])
    print('gam')
    print(gam)
    return ll, gam

#Eステップ
def e_step(data, pars):
    ll, gam = loglikelihood(data, pars)
    gammas = transpose(list(map(normalize, gam)))
    return gammas, ll

#Mステップ  pars = (pis, means, vars)
def m_step(data, gammas):
    ws = list(map(sum, gammas))
    pis = normalize(ws)
    means = [np.dot(g, data)/w for g, w in zip(gammas, ws)]
    vars = [np.dot(g, data**2)/w - (np.dot(g, data)/w)**2 for g, w in zip(gammas, ws)]
    #vars = [make_var(g, data, mu)/w for g, w, mu in zip(gammas, ws, means)]
    return pis, means, vars

#共分散
def make_var(gammas, data, mean):
    return np.sum([g * make_cov(x-mean) for g, x in zip(gammas, data)], axis=0)

def make_cov(x):
    nvar = n_variate(x)
    if not nvar:
        return x**2
    m = np.matrix(x)
    return m.reshape(nvar, 1) * m.reshape(1, nvar)

#n-変量
def n_variate(x):
    if isinstance(x, (list, np.ndarray)):
        return len(x)
    return 0  # univariate

#正規化
def normalize(lst):
    s = sum(lst)
    if s==0:
        s+=1
    return [x/s for x in lst]

#転置
def transpose(a):
    return list(map(list,zip(*a)))

def flatten(lst):
    if isinstance(lst[0], np.ndarray):
        lst = list(map(list, lst))
    return sum(lst, [])

def eigsorted(cov):
    vals, vecs = np.linalg.eigh(cov)
    order = vals.argsort()[::-1]
    return vals[order], vecs[:,order]

def flatten_with_any_depth(nested_list):
    """深さ優先探索の要領で入れ子のリストをフラットにする関数"""
    # フラットなリストとフリンジを用意
    flat_list = []
    fringe = [nested_list]

    while len(fringe) > 0:
        node = fringe.pop(0)
        # ノードがリストであれば子要素をフリンジに追加
        # リストでなければそのままフラットリストに追加
        if isinstance(node, list):
            fringe = node + fringe
        else:
            flat_list.append(node)

    return flat_list

